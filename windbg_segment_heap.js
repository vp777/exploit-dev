"use strict";

/*
//switch to paged pool (by default uses nonpagedpoolnx)
dx @$scriptContents.set_default_pool(2)

//switch to session pool of the current process
dx @$scriptContents.set_default_pool(4)
or
dx @$scriptContents.set_session_pool()

//switch to a segment heap by address, e.g. usermode implementation
dx @$scriptContents.set_default_heap(0x230b0470000)

//finds the heap associated with the address based on the segment allocator metadata, unlike the other set_default functions which require the actual segment heap address. Limitation: requires the address to lie within a segment block (all except large allocs)
dx @$scriptContents.set_default_heap_seg(0x230b0675300)


//********LFH functions************
//return the buckets that are either enabled or disabled
dx @$scriptContents.lfh_buckets_status()

//returns more detailed information about a specific bucket index
dx @$scriptContents.lfh_bucket_stats(8)

//********VS functions************
dx @$scriptContents.vs_dynamic_lookaside_info()
dx @$scriptContents.vs_dynamic_lookaside_bucket_info(0x390)
dx @$scriptContents.vs_delay_free_list()
dx @$scriptContents.vs_freechunktree_stats(0x390,0x3b0)
dx @$scriptContents.vs_decode_header_abs(0xffff9804c6838fe0)

//same as vs_decode_header_abs, but it doesn't require the address provided as arg to be the beginning of the chunk. It can be any address within the underlying chunk boundaries
dx @$scriptContents.vs_decode_header(0xffff9804c6838ff0)


//********Segment functions************
dx @$scriptContents.seg_free_blocks()

//segment details based on any address that lies within a segment
dx @$scriptContents.seg_segment_state(0xffff9804c6838fe8)

//********Large functions************
dx @$scriptContents.large_print_allocs()

------------

dx @$cursession.Processes.Where(x => x.KernelObject.Session != 0)

//all scripts function container
host.namespace.Debugger.State.DebuggerVariables.scriptContents

//specific script by name
Debugger.State.Scripts.windbg_segment_heap.Contents.vs_freechunktree_stats(0x80)

*/

let kernel_globals = {};
let session_pool_globals = {};
let globals = {};
let cached_types = {};
let debugging = false;
let logging_enabled = true;
const zero = host.parseInt64(0);


let log = host.diagnostics.debugLog;
let logln = function (e) {
    if (logging_enabled)
        log(e + '\n');
};

let debug_print = function (e) {
    if (debugging) {
        log("[debug] " + e + '\n');
    }
}

function to_hex(n){
    if (n===undefined)
        return "[to_hex: n===undefined]"
    return "0x" + n.toString(16);
}

function x(cmd){
    let output = "";
    
    //might be useful for some commands to let the caller handle exception
    try {
        for (let line of host.namespace.Debugger.Utility.Control.ExecuteCommand(cmd)){
            output += line + '\n';
        }
    } catch (err) {
        output = "";
    }
    return output.trim();
}

function read_mem(addr, element_size, n = 1){
    let elements = host.memory.readMemoryValues(addr, n, element_size);
    if (n == 1)
        return elements[0];
    
    return elements;
}

function create_mask(bits) {
    return 1..bitwiseShiftLeft(bits).subtract(1);
}

//maybe todo: add option to extract overlapping fields, e.g. _RTL_BALANCED_NODE.ParentValue&~3
function extract_field_64(value, type_name, field){
    let tp = get_type_parts(type_name);
    let current_type = get_type(tp.module, tp.type_name);
    let data;
    
    
    let field_base_offset = offsetof(type_name, field) % 8;
    let field_base_size = type_field_size(type_name, field);
    let field_mask = create_mask(field_base_size*8);
    
    if (field_base_offset + field_base_size > 8)
        throw Error("[extract_field_64] extraction field size bigger than the value size");
    
    value = value.bitwiseShiftRight(field_base_offset*8);
    
    let bit_info = type_field_bitinfo(type_name, field);
    if (bit_info === undefined) {
        value = value.bitwiseAnd(field_mask);
        if (field_base_size*8<=53){
            value = value.asNumber();
            debug_print(field + " is bytes, base sz: " + field_base_size);
        }
        return value;
    }

    field_mask = create_mask(bit_info.bit_len);
    value = value.bitwiseShiftRight(bit_info.bit_position);
    value = value.bitwiseAnd(field_mask);
    
    if (bit_info.bit_len<=53){
        value = value.asNumber();
        debug_print(field + " is bits, base sz: " + bit_info.bit_len);
    }
    
    return value;
}

function is_kernelspace_address(addr){
    return addr.bitwiseShiftRight(47).bitwiseAnd(0x1ffff) == 0x1ffff;
}

function is_userspace_address(addr){
    return addr.bitwiseShiftRight(47) === zero;
}

function verify_four_pagetable_levels() {
    let output = x("!pte 0");
    let output_lines = output.split("\n").splice(1);
    
    if (output_lines[0].indexOf("PXE") != 0){
        throw new Error(`[verify_four_pagetable_levels] error: currently assuming 4-level page table hierarchy, fix is_kernelspace_address/is_userspace_address probably some other places too cmd output: ${output}`);
    }
    
    return true;
}

function check_machine_configuration(){
    verify_four_pagetable_levels();
    
    //maybe add some other assumptions, e.g. vs header size
}

function create_typed_object(addr, type_name, module = null){
    if (module === null)
        module = globals.default_module;
    
    return host.createTypedObject(addr, module, type_name);
}

function create_pointer_object(addr, type_name, module = null){
    if (module === null)
        module = globals.default_module;
    
    return host.createPointerObject(addr, module, type_name);
}

function get_type_parts(type_name){
    let type_parts = type_name.split("!");
    let real_type_name = type_parts.pop();
    let module = type_parts.pop() || globals.default_module;
    
    return {
        module: module,
        type_name: real_type_name,
    }
}

function get_type(module, type_name){
    let full_name = module+"!"+type_name;
    if (full_name in cached_types)
        return cached_types[full_name];
    
    let type = host.getModuleType(module, type_name);
    cached_types[full_name] = type;
    return type;
}

function bittest(arr, arr_element_size, bit_test_position){
    let test_element_index = bit_test_position.divide(arr_element_size);
    let test_bit_index = bit_test_position.subtract(test_element_index.multiply(arr_element_size));
    //logln("element: " + to_hex(arr[test_element_index]) + " test_bit_index: " + to_hex(test_bit_index) + " bit_test_position: " + to_hex(bit_test_position));
    return arr[test_element_index].bitwiseAnd(1..bitwiseShiftLeft(test_bit_index)) != zero;
}

function bittest64(arr, bit_test_position){
    return bittest(arr, 64, bit_test_position);
}

function bitwiseNot(n){
    return 0..subtract(n).subtract(1);
}

function align(n, bits, align_up = true){
    let mask = 1..bitwiseShiftLeft(bits).subtract(1);
    
    if (align_up)
        n = n.add(mask);
    
    return n.bitwiseAnd(bitwiseNot(mask));
}

function offsetof(type_name, field){
    let tp = get_type_parts(type_name);
    let final_offset = 0;
    let current_type = get_type(tp.module, tp.type_name);
    for(let current_field_name of field.split(".")){
        let current_field = current_type.fields[current_field_name];
        final_offset += current_field.offset;
        current_type = current_field.type;
    }
    return final_offset;
}

function type_size(type_name){
    let type_parts = get_type_parts(type_name);
    
    let type = get_type(type_parts.module, type_parts.type_name);
    return type.size;
}

function get_type_field(type_name, field){
    let type_parts = get_type_parts(type_name);
    let current_type = get_type(type_parts.module, type_parts.type_name);
    let current_field;
    
    for(let current_field_name of field.split(".")){
        current_field = current_type.fields[current_field_name];
        current_type = current_field.type;
    }
    
    return current_field;
}

function type_field_bitinfo(type_name, field){
    let field_type = get_type_field(type_name, field).type;
    if (!field_type.isBitField)
        return undefined;
    
    return {
        bit_position: field_type.bitFieldPositions.lsb,
        bit_len: field_type.bitFieldPositions.length,
    };
}

function type_field_size(type_name, field){
    return get_type_field(type_name, field).type.size;
}

//eval_node will receive a node as argument and should return:
// -1: if we are looking for smaller value than the one held by the current node
// 0 : if the value is good
// 1 : if we are looking for bigger value
function* rbtree_iterator(root_node, eval_node){
    if (!root_node || root_node.address == zero)
        return;
    
    let node_queue = [{node:root_node, position:"root"}];
    
    while(node_queue.length>0){
        let current_node_info = node_queue.pop();
        let current_node = current_node_info.node;
        let eval_ret = eval_node(current_node);
        let eval_result = eval_ret.result;
        
        if (eval_result==0){
            if ("extra" in eval_ret)
                current_node_info.extra = eval_ret.extra;
            yield current_node_info;
        }
        
        if ((current_node.Left.address!=zero) && eval_result <= 0)
            node_queue.push({node:current_node.Left, position:"Left"});
        
        if ((current_node.Right.address!=zero) && eval_result >= 0)
            node_queue.push({node:current_node.Right, position:"Right"});
    }
}

function get_pool(pool_index){
    if (pool_index == -1)
        return globals.default_heap;
    
    if (pool_index >= 0 && pool_index<globals.pools.length)
        return globals.pools[pool_index];
    
    return get_current_session_pool();
}

function* _slist_iterator(slist_header) {
    if (slist_header == zero)
        return;
    
    let current_node_addr = slist_header.Region.bitwiseAnd(~15);
    while(current_node_addr != zero){
        yield current_node_addr;
        current_node_addr = read_mem(current_node_addr, 8); 
    }
}

function _list_entry_iterator(list_entry, type, field){
    return host.namespace.Debugger.Utility.Collections.FromListEntry(list_entry, type, field);
}

//we may have to call this function liberally, we could cache results in case execution wasnt resumed between the different calls
function _list_entry_address(list_entry_object, offset = 0){
    if (list_entry_object.Flink.address == zero){
        return zero;
    }
    return list_entry_object.Flink.Blink.address.subtract(offset);
}

function lfh_index2size(bucket_index){
    return globals.RtlpBucketBlockSizes[bucket_index]
}

function lfh_size2index(size){
    let index = (size+0xf)>>>4;
    //if (index>=globals.RtlpLfhBucketIndexMap.len) //add a similar check to avoid table overflow
    //    return -1;
    return globals.RtlpLfhBucketIndexMap[index];
}

function _lfh_decode_blockoffsets(lfh_subsegment){
    let decoded_blockoffsets = _list_entry_address(lfh_subsegment.ListEntry).bitwiseAnd(0xffffffff)>>>12;
    decoded_blockoffsets ^= lfh_subsegment.BlockOffsets.EncodedData;
    decoded_blockoffsets ^= globals.RtlpHpHeapGlobals.LfhKey.bitwiseAnd(0xffffffff);
    
    return {
        BlockSize: decoded_blockoffsets & 0xffff,
        FirstBlockOffset: decoded_blockoffsets>>>16,
    };
}

function _lfh_subsegment_free_blocks(subsegment){
   let block_count = subsegment.BlockCount;
   let free_count = subsegment.FreeCount;
   let block_offsets = _lfh_decode_blockoffsets(subsegment);
   let subsegment_address = _list_entry_address(subsegment.ListEntry)
   
   let current_bitmap;
   let n = 0;
   let free_blocks = [];
   let blocks_per_bitmap_element = type_field_size("_HEAP_LFH_SUBSEGMENT", "BlockBitmap")*8/2; //8 bits/byte, 2 bits per block

   for (let i=0;i<block_count;i++){
       if (i%blocks_per_bitmap_element==0) {
           current_bitmap = subsegment.BlockBitmap[i/blocks_per_bitmap_element]
       }
       if (current_bitmap.bitwiseAnd(1) == 0) {
            n += 1;
            free_blocks.push(subsegment_address.add(block_offsets.FirstBlockOffset + i*block_offsets.BlockSize));
       }
       
       current_bitmap = current_bitmap.bitwiseShiftRight(2);
   }
   
   return free_blocks;
}

function lfh_buckets_status(show_disabled = 1, pool_index = -1){
    const status_msg = [
        "enabled",
        "disabled",
    ][(show_disabled!=0)|0];
    
    let current_pool = get_pool(pool_index);
           
    let lfh_context = current_pool.segment_heap.LfhContext; 
    let lfh_context_addr = current_pool.segment_heap.address.add(offsetof("_SEGMENT_HEAP", "LfhContext"));
    let max_block_size = lfh_context.Config.MaxBlockSize;

    logln("------------LFH Info - " + current_pool.name + "----------");
    logln("LfhContext Address [_HEAP_LFH_CONTEXT]: " + to_hex(lfh_context_addr));
    logln("LFH Max Block Size: " + to_hex(max_block_size));
    for (let i = 0; i < lfh_context.Buckets.Count(); i++){
        let current_bucket = lfh_context.Buckets[i];
        let current_bucket_enabled = !(current_bucket.address.convertToNumber()&1);
        let current_bucket_size = lfh_index2size(i);
        
        if (current_bucket_size>=max_block_size)
            break;
        
        if (current_bucket_enabled ^ show_disabled)
            logln("Bucket[" + to_hex(i) + "] with size: " + to_hex(current_bucket_size) + " is " + status_msg);
    }
    logln("----------------------");
}

//if bucket_id is number, then it's interpreted as bucket index
//if it's string, then it's interpreted as bucket size, e.g. bucket_id="0x480"
function lfh_bucket_stats(bucket_id, pool_index = -1){
    const location_msgs = [
        "available",
        "full",
        "decomission"
    ];
    
    let current_pool = get_pool(pool_index);
    
    let bucket_index = bucket_id;
    if (typeof bucket_id === 'string')
        bucket_index = lfh_size2index(bucket_id|0);
    
    let lfh_context = current_pool.segment_heap.LfhContext;
    let lfh_context_addr = current_pool.segment_heap.address.add(offsetof("_SEGMENT_HEAP", "LfhContext"));
    let max_block_size = lfh_context.Config.MaxBlockSize;
    let max_affinity_slots = lfh_context.MaxAffinity;

    logln("------------LFH Bucket[" + to_hex(bucket_index) + "] Stats - " + current_pool.name + "----------");
    logln("LfhContext Address [_HEAP_LFH_CONTEXT]: " + to_hex(lfh_context_addr));

    let bucket = lfh_context.Buckets[bucket_index];
    let bucket_addr = lfh_context.Buckets[bucket_index].address;
    let bucket_enabled = !(bucket_addr.convertToNumber()&1);
    let bucket_size = lfh_index2size(bucket_index);
    
    if (bucket_size>=max_block_size){
        logln("Bucket size provided is bigger than LFH max block size: " + to_hex(max_block_size));
        return;
    }
    
    if (!bucket_enabled){
        logln("Bucket with index " + to_hex(bucket_index) + " and size " + to_hex(bucket_size) + " is disabled");
        return;
    }
    
    logln("Bucket address [_HEAP_LFH_BUCKET]: " + to_hex(bucket_addr));
    logln("Bucket size: " + to_hex(bucket_size));
    
    let affinity_indexes = read_mem(bucket.ProcAffinityMapping.address, 1, max_affinity_slots);
    let processed_affinity_indexes = [];
    for (let i = 0; i < max_affinity_slots; i++) {
        let current_affinity_index = affinity_indexes[i];
        if (processed_affinity_indexes.indexOf(current_affinity_index)!==-1){
            continue;
        }
        
        logln(`\n########## Affinity Slot ${current_affinity_index} ##########`);
        
        processed_affinity_indexes.push(current_affinity_index);
    
        let current_affinity_slot = bucket.AffinitySlots[current_affinity_index];
        let active_subsegment_addr = current_affinity_slot.ActiveSubsegment.Target.address.bitwiseShiftRight(12).bitwiseShiftLeft(12);
        
        if (active_subsegment_addr != zero){
            let active_subsegment = create_typed_object(active_subsegment_addr, "_HEAP_LFH_SUBSEGMENT");
            let free_blocks = _lfh_subsegment_free_blocks(active_subsegment);
            logln("ActiveSubsegment" + " free blocks: " + free_blocks.length + "/" + active_subsegment.BlockCount + " (" + to_hex(active_subsegment_addr) + ", " + to_hex(active_subsegment.Owner.address) + ", " + active_subsegment.Location + ")");
        
            //for (let free_block of free_blocks) {
            //    logln(to_hex(free_block));
            //}
        
            logln("Free block addresses: " + free_blocks);
        } else {
            logln("ActiveSubsegment is nul"); //this appears to be true in usermode segment heap implementation
        }
        
        logln("--------------------------");
        
        let counter = 0;
        for(let subsegment of _list_entry_iterator(current_affinity_slot.State.AvailableSubsegmentList, globals.default_module + "!_HEAP_LFH_SUBSEGMENT", "ListEntry")){
            let subsegment_address = _list_entry_address(subsegment.ListEntry);
            let free_blocks = _lfh_subsegment_free_blocks(subsegment);
            logln(`AvailableSubsegment[` + to_hex(counter) + "]" + " free blocks: " + free_blocks.length + "/" + subsegment.BlockCount + " (" + to_hex(subsegment_address) + ", " + to_hex(subsegment.Owner.address) + ", " + subsegment.Location + ")");
            counter += 1;
        }
    
    }
    
    logln("\n######## Bucket State ###########");
    let bucket_state = bucket.State;
    let counter = 0;
    for(let subsegment of _list_entry_iterator(bucket_state.AvailableSubsegmentList, globals.default_module + "!_HEAP_LFH_SUBSEGMENT", "ListEntry")){
        let subsegment_address = _list_entry_address(subsegment.ListEntry);
        let free_blocks = _lfh_subsegment_free_blocks(subsegment);
        logln("Bucket state AvailableSubsegment[" + to_hex(counter) + "] free blocks: " + free_blocks.length + "/" + subsegment.BlockCount + " (" + to_hex(subsegment_address) + ", " + to_hex(subsegment.Owner.address) + ", " + subsegment.Location + ")");
        counter += 1;
    }
}

function _vs_header_sanity_check(vs_header){
    let sane = true; 
    sane = sane && (vs_header.Allocated == 0 || vs_header.Allocated == 1);
    sane = sane && (vs_header.UnsafeSize < 0x4000 && vs_header.UnsafePrevSize < 0x4000);
    sane = sane && (vs_header.MemoryCost < align(vs_header.UnsafeSize*0x10, 12)/0x1000);
    return sane
}

//for completeness, we need to go through the underlying segment allocations and find where our address falls into
//false positive rate is 1/2^16
function _vs_is_subsegment_first_page(addr){
    let addr_page = addr.bitwiseAnd(~0xfff);
    let vs_sub = create_typed_object(addr_page, "_HEAP_VS_SUBSEGMENT");
    return vs_sub.Size.bitwiseXor(vs_sub.Signature).bitwiseXor(0x2bed) == zero;
}

//we can cache the chunk boundaries for a particular subsegment to speed up the execution of this function in case it's called often
//the cache lifetime might be good idea to be controlled by the caller
//fast_path should speed up execution, the function will only try to find the chunk boundaries starting from the previous page instead of the beginning of the subsegment
function vs_chunk_start(target_chunk_addr, fast_path = false){
    let current_chunk_addr;
    
    //add within the first condition: && _vs_header_sanity_check(target_chunk_addr.subtract(0x1000).bitwiseAnd(~0xfff).add(0xfe0))
    //the above condition would make it unlikely to fail the fastpath in case the target chunk spans over multiple pages
    if (fast_path && is_kernelspace_address(target_chunk_addr)){
        current_chunk_addr = target_chunk_addr.subtract(0x1000).bitwiseAnd(~0xfff).add(0xfe0);
        if (_vs_is_subsegment_first_page(target_chunk_addr))
            current_chunk_addr = target_chunk_addr.bitwiseAnd(~0xfff).add(align(type_size("_HEAP_VS_SUBSEGMENT"), 4)); //another way of calculating where the vs subsegment user blocks start is by doing: (((~subsegment->Size)+1)&0xfff)*16, maybe there is a better way
    } else {
        current_chunk_addr = seg_subsegment_start(target_chunk_addr).add(align(type_size("_HEAP_VS_SUBSEGMENT"), 4));
    }
    
    if (current_chunk_addr.compareTo(target_chunk_addr)>0){
        debug_print("[vs_chunk_start] Target address is bigger than the beginning of the lookup range: " + to_hex(current_chunk_addr));
        return undefined;
    }

    let loop_counter = 0;
    while(true){
        let header = vs_decode_header_abs(current_chunk_addr, true);
        let next_chunk_addr = current_chunk_addr.add(header.UnsafeSize*0x10);

        if (next_chunk_addr.compareTo(target_chunk_addr)>0)
            break;
        current_chunk_addr = next_chunk_addr;
        loop_counter+=1;
        /*
        if (loop_counter>0x500){
            logln("something is off, going out");
            return undefined;
        }
        */
    }
    return current_chunk_addr;
}

function _vs_test(target_chunk_addr, fast_path = false){
    let current_chunk_addr;
    
    //add within the first condition: && _vs_header_sanity_check(target_chunk_addr.subtract(0x1000).bitwiseAnd(~0xfff).add(0xfe0))
    //the above condition would make it unlikely to fail the fastpath in case the target chunk spans over multiple pages
    if (fast_path && is_kernelspace_address(target_chunk_addr)){
        current_chunk_addr = target_chunk_addr.subtract(0x1000).bitwiseAnd(~0xfff).add(0xfe0);
        if (_vs_is_subsegment_first_page(target_chunk_addr))
            current_chunk_addr = target_chunk_addr.bitwiseAnd(~0xfff).add(align(type_size("_HEAP_VS_SUBSEGMENT"), 4)); //another way of calculating where the vs subsegment user blocks start is by doing: (((~subsegment->Size)+1)&0xfff)*16, maybe there is a better way
    } else {
        current_chunk_addr = seg_subsegment_start(target_chunk_addr).add(align(type_size("_HEAP_VS_SUBSEGMENT"), 4));
    }
    
    if (current_chunk_addr.compareTo(target_chunk_addr)>0){
        debug_print("[vs_chunk_start] Target address is bigger than the beginning of the lookup range: " + to_hex(current_chunk_addr));
        return undefined;
    }
    
    let subsegment_end_address = current_chunk_addr.add(0x10000);

    while(true){
        let header = vs_decode_header_abs(current_chunk_addr, true);
        let pool_header_addr = current_chunk_addr.add(0x10);
        
        if (current_chunk_addr.bitwiseAnd(0xfff) == 0xfe0)
            pool_header_addr = pool_header_addr.add(0x10);
        
        if (header.UnsafeSize<0xfd0){
            let pool_header = create_pointer_object(pool_header_addr, "_POOL_HEADER*");
            logln("Current chunk: " + to_hex(current_chunk_addr) + " diff: " +to_hex(header.UnsafeSize.subtract(pool_header.BlockSize)));
        }
        
        let next_chunk_addr = current_chunk_addr.add(header.UnsafeSize*0x10);

        if (next_chunk_addr.compareTo(subsegment_end_address)>=0)
            break;
        current_chunk_addr = next_chunk_addr;
    }
    return current_chunk_addr;
}

function vs_decode_header(vs_header_addr, fast_path = false, partial_decode=false){
    let actual_vs_header_addr = vs_chunk_start(vs_header_addr, fast_path);
    
    if (actual_vs_header_addr == undefined){
        logln("Couldnt identify the beginning of the chunk");
        return;
    }
    
    if (actual_vs_header_addr.compareTo(vs_header_addr)!=0){
        logln("Original adresss:             " + to_hex(vs_header_addr));
        logln("Identified vs header address: " + to_hex(actual_vs_header_addr));
    }
    return vs_decode_header_abs(actual_vs_header_addr, partial_decode);
}

function vs_decode_header_abs(vs_header_addr, partial_decode=false){
    if (typeof vs_header_addr === 'string'){
        vs_header_addr = host.parseInt64(vs_header_addr);
    }
    
    let decoded_header = vs_header_addr;
    decoded_header = decoded_header.bitwiseXor(host.memory.readMemoryValues(vs_header_addr, 1, 8)[0]);
    decoded_header = decoded_header.bitwiseXor(globals.RtlpHpHeapGlobals.HeapKey);
    
    //instantiating objects from js bytes would have been useful feature for the windbg js engine. Unfortunately this doesntt seem to exist yet, so we recreate the _HEAP_VS_CHUNK_HEADER ourselves
    //let memory_cost = decoded_header.bitwiseAnd(0xffff);
    let memory_cost = extract_field_64(decoded_header, "_HEAP_VS_CHUNK_HEADER", "Sizes.MemoryCost");
    //let unsafe_size = decoded_header.bitwiseShiftRight(16) & 0xffff;
    let unsafe_size = extract_field_64(decoded_header, "_HEAP_VS_CHUNK_HEADER", "Sizes.UnsafeSize");
    
    //saves time, within this script we don't really use any of the other fields
    if (partial_decode)
        return {
            MemoryCost: memory_cost,
            UnsafeSize: unsafe_size,
        };
    
    //let unsafe_prev_size = decoded_header.bitwiseShiftRight(32) & 0xffff;
    let unsafe_prev_size = extract_field_64(decoded_header, "_HEAP_VS_CHUNK_HEADER", "Sizes.UnsafePrevSize");

    //let allocated = decoded_header.bitwiseShiftRight(48) & 0xff;
    let allocated = extract_field_64(decoded_header, "_HEAP_VS_CHUNK_HEADER", "Sizes.Allocated");
    let extra = decoded_header.bitwiseShiftRight(56);
    
    if (allocated) {
        let allocated_chunk_bits = host.memory.readMemoryValues(vs_header_addr.add(offsetof("_HEAP_VS_CHUNK_HEADER", "EncodedSegmentPageOffset")), 1, 4)[0];
        let segment_page_offset  = vs_header_addr.bitwiseAnd(0xff);
        segment_page_offset ^= allocated_chunk_bits & 0xff;
        segment_page_offset ^= globals.RtlpHpHeapGlobals.HeapKey.bitwiseAnd(0xff)
        
        
        //let unused_bytes = (allocated_chunk_bits >>> 8) & 1;
        let unused_bytes = extract_field_64(allocated_chunk_bits, "_HEAP_VS_CHUNK_HEADER", "UnusedBytes");
        //let skip_during_walk = (allocated_chunk_bits >>> 9) & 1;
        let skip_during_walk = extract_field_64(allocated_chunk_bits, "_HEAP_VS_CHUNK_HEADER", "SkipDuringWalk");
        
        //these two require properly setting the segment heap before calling the vs decode
        let dynamic_lookaside = vs_dynamic_lookaside_block(vs_header_addr, unsafe_size*0x10);
        let delay_free_list = vs_delay_free_list_block(vs_header_addr);
        
        /*
        if (globals.kernelmode){
            let pool_header_addr = vs_header_addr.add(0x10);
            if (vs_header_addr.bitwiseAnd(0xfff) == 0xfe0){
                pool_header_addr = pool_header_addr.add(0x10);
                probably_delay_free_list = unsafe_size<0x100 && globals.is_valid_address(read_mem(vs_header_addr.add(0x10), 8));
            }
            let pool_header = create_typed_object(pool_header_addr, "_POOL_HEADER");

            //test whether the pool header appears to be corrupted, ie out of sync with vs header size
            if (unsafe_size>=0x100 || (pool_header.BlockSize < unsafe_size && pool_header.BlockSize+3>unsafe_size)) {
                potentially_delay_free_list = false;
                potentially_dynamic_lookaside = false;
            }
        }
        */
        
        return {
            MemoryCost: memory_cost,
            UnsafeSize: unsafe_size,
            UnsafePrevSize: unsafe_prev_size,
            Allocated: allocated,
            Extra: extra,
            
            //allocated chunk headers
            SegmentPageOffset: segment_page_offset, 
            UnusedBytes: unused_bytes, //todo: print the actual unused bytes
            SkipDuringWalk: skip_during_walk,
            AllocatedChunkBits: allocated_chunk_bits,
            
            //specify whether the chunk is within either dynamic lookaside or delay free list
            DelayFreeList: delay_free_list,
            DynamicLookaside: dynamic_lookaside,
        };
    }
    
    //handle the fields in case the chunk is free
    let left = read_mem(vs_header_addr.add(8), 8);
    let right = read_mem(vs_header_addr.add(0x10), 8);
    let parent_val = read_mem(vs_header_addr.add(0x18), 8);
    
    //extract_field_64(parent_val, "_RTL_BALANCED_NODE", "Red");
    let parent = parent_val.bitwiseAnd(~3);
    let red = parent_val.bitwiseAnd(1);
    let balance = parent_val.bitwiseAnd(3);

    return {
        MemoryCost: memory_cost,
        UnsafeSize: unsafe_size,
        UnsafePrevSize: unsafe_prev_size,
        Allocated: allocated,
        Extra: extra,
        
        //freed chunk headers
        Left: left,
        Right: right,
        Parent: parent,
        Red: red,
        Balance: balance,
    };
}

function vs_freechunktree_stats(min_size = 0, max_size = 0xfffff0, pool_index = -1, root_node = null){
    let sizes_freq = [];

    let current_pool = get_pool(pool_index);

    if (!root_node)
        root_node =  current_pool.segment_heap.VsContext.FreeChunkTree.Root;
    
    let free_chunk_tree_addr = current_pool.segment_heap.address.add(offsetof("_SEGMENT_HEAP", "VsContext.FreeChunkTree"));
    let encoded_root = current_pool.segment_heap.VsContext.FreeChunkTree.Encoded;
        
    if (encoded_root)
        root_node = create_pointer_object(root_node.address.bitwiseXor(free_chunk_tree_addr), "_RTL_BALANCED_NODE*");
    
    if (root_node.address == zero){
        logln("FreeChunkTree is empty for " + current_pool.name);
        return;
    }

    let total_size = 0;
    let n = 0;
    
    logln("----------FreeChunkTree Stats - " + current_pool.name + " -------------");
    logln("FreeChunkTree address [_RTL_RB_TREE]: " + to_hex(free_chunk_tree_addr));
    logln("Filtering sizes " + to_hex(min_size) + "-" + to_hex(max_size));
    logln("[%Size%] %Frequency%:")
    
    let node_evaluator = function (node) {
        let header = vs_decode_header_abs(node.address.subtract(8), true)
        let sz = header.UnsafeSize * 0x10;
        let result = 0;
        if (sz < min_size)
            result = 1;
        else if (sz > max_size)
            result = -1;
        return {result: result, extra: header};
    };
    
    for (let node_info of rbtree_iterator(root_node, node_evaluator)){
        let current_node = node_info.node;
        let current_node_vs_header = node_info.extra
        let current_node_size = current_node_vs_header.UnsafeSize * 16;
        let current_position = node_info.position;

        sizes_freq[current_node_size] = (sizes_freq[current_node_size]||0) + 1;
        total_size += current_node_size;
        n++;
        logln(to_hex(current_node.address) + " sz: " + to_hex(current_node_size) + " cost:" + to_hex(current_node_vs_header.MemoryCost) + " d:" + current_position);
        
        /*
        if (n%100==0)
            logln(n);
        */
    }

    let col = 0;
    for (const [i, value] of sizes_freq.entries()) {
        if (value) {
            log(("[" + to_hex(i) + "] " + to_hex(value)).padEnd(16) + "\t\t");
            col++;
            if (col==3){
                logln("");
                col = 0;
            }
        }
    }
    logln("\n++++++++++++++++++++++");
    logln("Total number of chunks: " + to_hex(n));
    logln("Total size: " + to_hex(total_size) + " (" + (total_size/(1024*1024)).toFixed(2) + " mb)");
    logln("------------------------\n");
}

//vs_dynamic_lookaside_* assume lookaside start index begins at 0x21, might be more accurate to use something something: lfh_size2index(LfhContext.Config.MaxBlockSize+sizeof(generic_handler_header e.g. POOL_HEADER))
//dynanmic lookaside chunk size to index
function vs_dynamic_lookaside_s2i(sz) {
    if (sz < globals.dynamic_lookaside_min_size){
        debug_print(`vs_dynamic_lookaside_s2i: returning -1, ${to_hex(sz)} < ${to_hex(globals.dynamic_lookaside_min_size)}:dynamic_lookaside_min_size`);
        return -1;
    }
    
    if (sz > globals.dynamic_lookaside_max_size){
        debug_print(`vs_dynamic_lookaside_s2i: returning -1, ${to_hex(sz)} > ${to_hex(globals.dynamic_lookaside_max_size)}:dynamic_lookaside_max_size`);
        return -1;
    }
    
    return lfh_size2index(sz)-0x21;
}

//dynanmic lookaside index to chunk size
function vs_dynamic_lookaside_i2s(bucket_index){
    return globals.RtlpBucketBlockSizes[0x21 + bucket_index]
}

function vs_dynamic_lookaside_bucket_info(size, pool_index = -1){
    let current_pool = get_pool(pool_index);
    let dl_blocks = []
    
    let dynamic_lookaside = current_pool.dynamic_lookaside;
    
    if (dynamic_lookaside.address==zero) {
        logln("[!] Dynamic lookaside disabled for " + current_pool.name);
        return [];
    }
    
    let enable_bucket_bitmap = dynamic_lookaside.EnabledBucketBitmap;
    
    logln("------Dynamic Lookaside Info - " + current_pool.name + "--------");
    logln("Dynamic Lookaside Address [_RTL_DYNAMIC_LOOKASIDE]: " + to_hex(dynamic_lookaside.address));
    logln("Enable Bucket Bitmap: " + to_hex(enable_bucket_bitmap));
    logln("-------------------------");
    
    let i = vs_dynamic_lookaside_s2i(size);

    if (i<0 || i>=dynamic_lookaside.Buckets.Count())
        return [];
    
    let current_bucket_depth = dynamic_lookaside.Buckets[i].Depth;
    let current_bucket_entries = dynamic_lookaside.Buckets[i].ListHead.HeaderX64.Depth;
    let current_bucket_bitmap_activity = enable_bucket_bitmap.bitwiseShiftRight(i).bitwiseAnd(1) == 1;
    let current_bucket_address = dynamic_lookaside.address.add(offsetof("_RTL_DYNAMIC_LOOKASIDE", "Buckets")).add(i*type_size("_RTL_LOOKASIDE"));

    let bucket_activity_code = ((current_bucket_depth!=0)<<1) | (current_bucket_entries!=0);

    if (!bucket_activity_code){
        return [];
    }

    let current_bucket_next_chunk = dynamic_lookaside.Buckets[i].ListHead.Region;
    let current_bucket_size = vs_dynamic_lookaside_i2s(i);
    let current_bucket_maximum_depth = dynamic_lookaside.Buckets[i].MaximumDepth;
    
    for(let addr of _slist_iterator(dynamic_lookaside.Buckets[i].ListHead)){
        logln(to_hex(addr));
        dl_blocks.push(addr);
    }
    
    return dl_blocks;
}

function vs_dynamic_lookaside_info(pool_index = -1){
    //bitmap depth nentries
    //0 0 inactive, empty 
    //0 1 inactive, but the bucket still has chunks
    //1 0 active, but no chunks currently in the bucket
    //1 1 active and contains chunks
    const bucket_activity_msgs = [
        "inactive",
        "inactive, but the bucket still has chunks",
        "active, but no chunks currently in the bucket",
        "active and contains chunks"
    ];

    let current_pool = get_pool(pool_index);
    
    let dynamic_lookaside = current_pool.dynamic_lookaside;
    
    
    if (dynamic_lookaside.address==zero) {
        logln("[!] Dynamic lookaside disabled for " + current_pool.name);
        return;
    }
    
    let enable_bucket_bitmap = dynamic_lookaside.EnabledBucketBitmap;
    
    logln("------Dynamic Lookaside Info - " + current_pool.name + "--------");
    logln("Dynamic Lookaside Address [_RTL_DYNAMIC_LOOKASIDE]: " + to_hex(dynamic_lookaside.address));
    logln("Enable Bucket Bitmap: " + to_hex(enable_bucket_bitmap));
    logln("-------------------------");
    
    for (let i=0;i<dynamic_lookaside.Buckets.Count();i++){
        let current_bucket_depth = dynamic_lookaside.Buckets[i].Depth;
        let current_bucket_entries = dynamic_lookaside.Buckets[i].ListHead.HeaderX64.Depth;
        let current_bucket_bitmap_activity = enable_bucket_bitmap.bitwiseShiftRight(i).bitwiseAnd(1) == 1;
        let current_bucket_address = dynamic_lookaside.address.add(offsetof("_RTL_DYNAMIC_LOOKASIDE", "Buckets")).add(i*type_size("_RTL_LOOKASIDE"));
 
        let bucket_activity_code = ((current_bucket_depth!=0)<<1) | (current_bucket_entries!=0);
        let bucket_activity_msg = bucket_activity_msgs[bucket_activity_code];

        if (!bucket_activity_code){
            continue;
        }

        let current_bucket_next_chunk = dynamic_lookaside.Buckets[i].ListHead.Region;
        let current_bucket_size = vs_dynamic_lookaside_i2s(i);
        let current_bucket_maximum_depth = dynamic_lookaside.Buckets[i].MaximumDepth;

        logln("Bucket address [_RTL_LOOKASIDE]: " + to_hex(current_bucket_address));
        logln("Size " + to_hex(current_bucket_size) + ", index: " + to_hex(i) + " [" + bucket_activity_msg + "]");
        if ((current_bucket_depth!=0) ^ current_bucket_bitmap_activity) {
            logln("*** Note: bitmap activity != depth indication, check it out"); //never seen this, ifaict not possible
        }
        logln("Current instance maximum depth: " + to_hex(current_bucket_depth) + " (max depth: " + to_hex(current_bucket_maximum_depth) + ")");
        logln("Number of entries in the list: " + to_hex(current_bucket_entries));
        logln("Head of the list: " + to_hex(current_bucket_next_chunk));
        logln("++++++++++++++++++++");
    }
    logln("");
}

function vs_dynamic_lookaside_vssize_blocks(vs_size) {
    let lowest_size_estimation = vs_size-0x20;
    let higest_size_estimation = vs_size-0x10;
    
    let dl_blocks = [];
    let prev_index = -1;
    
    logging_enabled = false;
    for (let current_size = lowest_size_estimation; current_size <= higest_size_estimation; current_size += 0x10){
        let current_index = vs_dynamic_lookaside_s2i(current_size);
        if (current_index < 0 || current_index == prev_index)
            continue;
        
        dl_blocks.push(...vs_dynamic_lookaside_bucket_info(current_size));
        prev_index = current_index;
    }
    
    logging_enabled = true;
    return dl_blocks;
}

//returns true if a block belongs to the dynamic lookaside
//block addr is expected to be the beginning of the chunk
function vs_dynamic_lookaside_block(chunk_addr, vssize) {
    let chunk_start = chunk_addr;
    let chunk_end = chunk_addr.add(vssize);

    for (let dl_chunk_addr of vs_dynamic_lookaside_vssize_blocks(vssize)){
        debug_print("currnet: " + to_hex(dl_chunk_addr));
        if (chunk_start.compareTo(dl_chunk_addr) <= 0 && chunk_end.compareTo(dl_chunk_addr) >= 0)
            return true;
    }
    
    return false;
}

function _vs_delay_free_list_info(pool_index = -1){
    let current_pool = get_pool(pool_index);
    let vs_context = current_pool.segment_heap.VsContext;
    
    if (!vs_context.Config.Flags.EnableDelayFree){
        return undefined;
    }

    let dfc = current_pool.segment_heap.VsContext.DelayFreeContext;
    let dfc_addr = current_pool.segment_heap.address.add(offsetof("_SEGMENT_HEAP", "VsContext.DelayFreeContext"));

    let depth = dfc.ListHead.HeaderX64.Depth;
    let delay_free_list = [];
    let current_addr = dfc.ListHead.HeaderX64.NextEntry.bitwiseShiftLeft(4);
    for (let i=0;i<depth;i++){
        delay_free_list.push(current_addr);
        current_addr = host.memory.readMemoryValues(current_addr, 1, 8)[0];
    }
    return {
        dfc_addr: dfc_addr,
        depth: depth,
        delay_free_list: delay_free_list,
    }
}

function vs_delay_free_list(pool_index = -1){
    let current_pool = get_pool(pool_index);
    let vs_context = current_pool.segment_heap.VsContext;
    let dfli = _vs_delay_free_list_info(pool_index);
    
    if (dfli === undefined){
        logln("The delayed free list is disalbed for " + current_pool.name)
        return;
    }

    logln("Delayed Free List Address: [_SLIST_HEADER]" + to_hex(dfli.dfc_addr) + " Depth: " + to_hex(dfli.depth));
    
    for (let current_addr of dfli.delay_free_list){
        let extra_data = "";
        if (globals.kernelmode && current_addr.bitwiseAnd(0xfff) == 0xff0 && vs_context.Config.Flags.PageAlignLargeAllocs){
            let pool_header = create_typed_object(current_addr.add(0x10), "_POOL_HEADER");
            extra_data = " ph_sz: " + to_hex(pool_header.BlockSize*0x10);
        }
        
        let header = vs_decode_header_abs(current_addr.subtract(0x10), true);
        logln("addr: " + to_hex(current_addr) + " sz: " + to_hex(header.UnsafeSize*0x10) + extra_data);
    }
}

function vs_delay_free_list_block(block_addr, pool_index = -1){
    let current_pool = get_pool(pool_index);
    let vs_context = current_pool.segment_heap.VsContext;
    let dfli = _vs_delay_free_list_info(pool_index);
    
    if (dfli === undefined){
        return false;
    }
    
    let block_addr_adjusted = block_addr.add(0x10); //adjusted to point to the potential delay free list address
        
    for (let dfl_block_addr of dfli.delay_free_list){
        if (block_addr_adjusted.compareTo(dfl_block_addr) == 0)
            return true;
    }
    
    return false;
}

function address_allocation_type(addr){
    let heap_manager = globals.heap_manager;
    let alloc_tracker = heap_manager.AllocTracker;
    let alloc_tracker_bitmap = heap_manager.AllocTracker.AllocTrackerBitmap;
    
    let address_bitmap_index = 2*addr.subtract(alloc_tracker.BaseAddress).bitwiseShiftRight(20).asNumber();
    
    if (!bittest64(alloc_tracker_bitmap.CommitDirectory, address_bitmap_index>>>30)){
        //logln("failed first: " + to_hex(alloc_tracker_bitmap.CommitDirectory.address));
        return 0;
    }
    
    if (!bittest64(alloc_tracker_bitmap.CommitBitmap, address_bitmap_index>>>15)){
        //logln("failed sec: " + to_hex(alloc_tracker_bitmap.CommitBitmap.address));
        return 0;
    }
    
    return alloc_tracker_bitmap.UserBitmap[address_bitmap_index>>>6].bitwiseShiftRight(address_bitmap_index&0x3f).bitwiseAnd(3);
}

function _seg_decode_signature(addr, seg_context_addr = 0){
    let signature = addr;
    signature = signature.bitwiseXor(globals.RtlpHpHeapGlobals.HeapKey);
    signature = signature.bitwiseXor(read_mem(addr.add(offsetof("_HEAP_PAGE_SEGMENT", "Signature")), 8));
    signature = signature.bitwiseXor(seg_context_addr);
    signature = signature.bitwiseXor(host.parseInt64("0xA2E64EADA2E64EAD"));
    return signature;
}

function is_large_alloc(addr){
    let type = address_allocation_type(addr);
    return type == 0 || type == 3;
}

function _seg_context_index(addr){
    let type = address_allocation_type(addr);
    if (type == 0 || type == 3)
        return -1;
    
    return type-1;
}

function _seg_get_address_segment(addr){
    let seg_context_index = _seg_context_index(addr);
    let page_segment = 0;
    let seg_context_address;
    
    if (seg_context_index == -1){
        return undefined;
    }
    
    let expected_seg_context_mask = globals.default_heap.segment_heap.SegContexts[seg_context_index].SegmentMask;

    page_segment = addr.bitwiseAnd(expected_seg_context_mask);
    
    seg_context_address = _seg_decode_signature(page_segment, 0);
    if (!globals.is_valid_address(seg_context_address)) {
        throw new Error(`[_seg_get_address_segment] couldnt get the segment context of ${addr}, invalid address: ${to_hex(seg_context_address)}`);
    }
    
    return {
        seg_context: create_pointer_object(seg_context_address, "_HEAP_SEG_CONTEXT*"),
        page_segment: create_pointer_object(page_segment, "_HEAP_PAGE_SEGMENT*"),
    };
}

function _seg_descriptor_is_first(desc){
    return (desc.RangeFlags & 2) != 0;
}

function _seg_segment_info_blind(addr){
    let random_pool = kernel_globals.pools[1]; //we could just hardcode the segment masks
    let page_segment = 0;
    let seg_context_address;
    
    for (let a_seg_context of random_pool.segment_heap.SegContexts){
        let segment_mask = a_seg_context.SegmentMask;        
        page_segment = addr.bitwiseAnd(segment_mask);
        
        try {
            seg_context_address = _seg_decode_signature(page_segment, 0);
            if (globals.is_valid_address(seg_context_address)) {
                return {
                    seg_context: create_pointer_object(seg_context_address, "_HEAP_SEG_CONTEXT*"),
                    page_segment: create_pointer_object(page_segment, "_HEAP_PAGE_SEGMENT*"),
                };
            }
        } catch (err) {}
    }
    
    return undefined;
    
}

function seg_page_descriptor(addr, seg_context = null, page_segment = null) {
    if (seg_context === null) {
        let seg_info = seg_get_seg_info(addr);
        
        page_segment = seg_info.page_segment;
        seg_context = seg_info.seg_context;
    }
    
    let seg_desc_addr = page_segment.address.add(addr.bitwiseShiftRight(seg_context.UnitShift).bitwiseShiftLeft(seg_context.UnitShift).subtract(page_segment.address).bitwiseShiftRight(seg_context.UnitShift).multiply(type_size("_HEAP_PAGE_RANGE_DESCRIPTOR")));
    
    return create_pointer_object(seg_desc_addr, "_HEAP_PAGE_RANGE_DESCRIPTOR*");
}

function seg_subsegment_descriptor(addr, seg_context = null, page_segment = null) {
    if (seg_context === null) {
        let seg_info = seg_get_seg_info(addr);
        
        page_segment = seg_info.page_segment;
        seg_context = seg_info.seg_context;
    }
    
    let addr_page_descriptor = seg_page_descriptor(addr, seg_context, page_segment);
    let subseg_desc_addr;
    
    if (_seg_descriptor_is_first(addr_page_descriptor)) {
        subseg_desc_addr = addr_page_descriptor.address;
    } else{
        subseg_desc_addr = addr_page_descriptor.address.subtract(addr_page_descriptor.UnitOffset.multiply(type_size("_HEAP_PAGE_RANGE_DESCRIPTOR")));
    }

    return create_pointer_object(subseg_desc_addr, "_HEAP_PAGE_RANGE_DESCRIPTOR*");
}

function seg_subsegment_start(addr, seg_context = null, page_segment = null) {
    if (seg_context === null) {
        let seg_info = seg_get_seg_info(addr);
        
        page_segment = seg_info.page_segment;
        seg_context = seg_info.seg_context;
    }
    
    let subseg_descriptor = seg_subsegment_descriptor(addr, seg_context, page_segment);
    return seg_descriptor_block_address(subseg_descriptor.address, seg_context, page_segment);
}

function seg_descriptor_block_address(desc_addr, seg_context = null, page_segment = null){
    if (seg_context === null) {
        let seg_info = seg_get_seg_info(desc_addr);
        
        page_segment = seg_info.page_segment;
        seg_context = seg_info.seg_context;
    }
    
    let desc_index = desc_addr.subtract(page_segment.address).divide(type_size("_HEAP_PAGE_RANGE_DESCRIPTOR"));
    return page_segment.address.add(desc_index.bitwiseShiftLeft(seg_context.UnitShift));
}

function _seg_range_flags_to_allocator(range_flags){    
    if (!(range_flags & 1))
        return "FREE";
    
    if ((range_flags & 0xc) == 8)
        return "LFH";
    
    if ((range_flags & 0xc) == 0xc)
        return "VS";
    
    return "Segment";
}

function seg_get_seg_info(addr){
    let seg_info = undefined;
    
    try {
        seg_info = _seg_get_address_segment(addr);
    } catch (err) {
        seg_info = undefined;
    }     
    
    if (seg_info === undefined) {
        //logln("Couldn't get segment info based on alloc tracker, did you use the set_* to set the default heap? Trying to bruteforce the seginfo");
        seg_info = _seg_segment_info_blind(addr);
    }
    
    if (seg_info === undefined) {
        return undefined;
    }
    
    return seg_info;
}

/*
*********** Segment Heap State Associated With 0xffff938fd3ccd000 ******************
Unit Size: 0x1000
------------------------
address: 0xffff938fd3c02000	|	units: 0x41	|	range flags: 0xb	|	state: LFH
address: 0xffff938fd3c43000	|	units: 0x10	|	range flags: 0x3	|	state: Segment
address: 0xffff938fd3c53000	|	units: 0x10	|	range flags: 0x3	|	state: Segment
address: 0xffff938fd3c63000	|	units: 0x10	|	range flags: 0x3	|	state: Segment
address: 0xffff938fd3c73000	|	units: 0x10	|	range flags: 0x3	|	state: Segment
address: 0xffff938fd3c83000	|	units: 0x21	|	range flags: 0xb	|	state: LFH
address: 0xffff938fd3ca4000	|	units: 0x9	|	range flags: 0x3	|	state: Segment
address: 0xffff938fd3cad000	|	units: 0x20	|	range flags: 0x3	|	state: Segment
address: 0xffff938fd3ccd000	|	units: 0x11	|	range flags: 0xf	|	state: VS
address: 0xffff938fd3cde000	|	units: 0x22	|	range flags: 0x2	|	state: FREE
*/
function seg_segment_state(addr){
    let seg_info = seg_get_seg_info(addr);
    
    if (seg_info === undefined){
        logln("Error: " + to_hex(addr) + " doesn't appear to have segment heap as backend");
        return;
    }
    
    let seg_context = seg_info.seg_context;
    let page_segment = seg_info.page_segment;
    let pages_per_unit = 1<<seg_context.PagesPerUnitShift;
    
    logln("*********** Segment Heap State Associated With " + to_hex(addr) + " ******************");
    logln("SegContext: " + to_hex(seg_context.address));
    logln("Unit Size: " + to_hex(pages_per_unit*0x1000));
    logln("------------------------");
    
    const desc_array_addr = page_segment.address.add(offsetof("_HEAP_PAGE_SEGMENT", "DescArray"));
    let i=seg_context.FirstDescriptorIndex;
    while(i<page_segment.DescArray.Count()){
        let current_descriptor = page_segment.DescArray[i];
        let current_descriptor_addr = desc_array_addr.add(i*type_size("_HEAP_PAGE_RANGE_DESCRIPTOR"));
        let current_units = current_descriptor.UnitSize;
        let current_block_address = page_segment.address.add(i*0x1000*pages_per_unit);
        
        logln(`desc[${to_hex(i)}]:\t${to_hex(current_descriptor_addr)}\t|\tblock address: ` + to_hex(current_block_address) + "\t|\tunits: " + to_hex(current_units) + "\t|\trange flags: " + to_hex(current_descriptor.RangeFlags) + "\t|\tstate: " + _seg_range_flags_to_allocator(current_descriptor.RangeFlags));
        
        i += current_units;
    }
}

/*
********* Segment free blocks for: NonPagedPoolNx *************
----------SegContext 0 - Unit Size: 0x1000---------
address: 0xffff938fd3bf9000	|	units: 0x7	|	range flags: 0x2	|	state: FREE
address: 0xffff938fd3ccd000	|	units: 0x33	|	range flags: 0x2	|	state: FREE

----------SegContext 1 - Unit Size: 0x10000---------
address: 0xffff938fd0c60000	|	units: 0x3a	|	range flags: 0x2	|	state: FREE
address: 0xffff938fd4480000	|	units: 0xb8	|	range flags: 0x2	|	state: FREE
address: 0xffff938fd0ab0000	|	units: 0xa	|	range flags: 0x2	|	state: FREE

@$scriptContents.seg_free_blocks()
*/
function seg_free_blocks(min_size = 0, max_size = 0xffffffffffff, pool_index = -1){
    let current_pool = get_pool(pool_index);
    let seg_contexts = current_pool.segment_heap.SegContexts;
    
    logln("********* Segment free blocks for: " + current_pool.name + " ***********");
    logln("Filtering sizes " + to_hex(min_size) + "-" + to_hex(max_size));

    let current_seg_context_addr = current_pool.segment_heap.address.add(offsetof("_SEGMENT_HEAP", "SegContexts"));
    for(let i=0;i<seg_contexts.Count();i++,current_seg_context_addr = current_seg_context_addr.add(type_size("_HEAP_SEG_CONTEXT"))){
        let current_seg_context = seg_contexts[i];
        let pages_per_unit = 1<<current_seg_context.PagesPerUnitShift;
        let bytes_per_unit = pages_per_unit*0x1000;
        logln(`----------SegContext[${i}] - Unit Size: ${to_hex(bytes_per_unit)}---------`);

        let root_node = current_seg_context.FreePageRanges.Root;
        if (current_seg_context.FreePageRanges.Encoded){
            let root_node_addr = root_node.address.bitwiseXor(current_seg_context_addr.add(offsetof("_HEAP_SEG_CONTEXT", "FreePageRanges.Root")));
            root_node = create_pointer_object(root_node_addr, "_RTL_BALANCED_NODE*");
            throw new Error(`[seg_free_blocks] not really a problem, encoded pointer found, remove the line that throws`);
        }
        
        let node_evaluator = function (node) {
            let prd = create_pointer_object(node.address, "_HEAP_PAGE_RANGE_DESCRIPTOR*");
            let sz = prd.UnitSize * bytes_per_unit;
            let result = 0;
            if (sz < min_size)
                result = 1;
            else if (sz > max_size)
                result = -1;
            return {result: result, extra: prd};
        };
        
        for (let node_info of rbtree_iterator(root_node, node_evaluator)){
            let prd = node_info.extra;
            let position = node_info.position;
            let page_segment = prd.address.bitwiseAnd(current_seg_context.SegmentMask);
            let descriptor_index = prd.address.subtract(page_segment)/type_size("_HEAP_PAGE_RANGE_DESCRIPTOR");
            let current_block_address = page_segment.add(descriptor_index*bytes_per_unit);
            let current_units = prd.UnitSize;
            
            logln("address: " + to_hex(current_block_address) + "\t|\tunits: " + to_hex(current_units) + "\t|\trange flags: " + to_hex(prd.RangeFlags) + "\t|\tstate: " + _seg_range_flags_to_allocator(prd.RangeFlags));

        }
        
        logln("");
    }
}

function large_print_allocs(pool_index = -1, min_size = 0, max_size = 0xffffffffffff){
    let current_pool = get_pool(pool_index);
    
    let root_node = current_pool.segment_heap.LargeAllocMetadata.Root;
    
    let large_tree_addr = current_pool.segment_heap.address.add(offsetof("_SEGMENT_HEAP", "LargeAllocMetadata"));
    let encoded_root = current_pool.segment_heap.LargeAllocMetadata.Encoded;
        
    if (encoded_root) {
        root_node = create_pointer_object(root_node.address.bitwiseXor(large_tree_addr), "_RTL_BALANCED_NODE*");
    }
    
    if (root_node.address == zero){
        logln("No large allocations for " + current_pool.name);
        return;
    }

    let total_size = 0;
    let n = 0;
    
    logln("----------Large Stats - " + current_pool.name + " -------------");
    logln("LargeAllocMetadata address [_RTL_RB_TREE]: " + to_hex(large_tree_addr));
    logln("Filtering sizes " + to_hex(min_size) + "-" + to_hex(max_size));

    let node_evaluator = function (node) {
        let large_alloc_data = create_pointer_object(node.address, "_HEAP_LARGE_ALLOC_DATA*");
        let sz = large_alloc_data.AllocatedPages * 0x1000;
        let result = 0;
        if (sz < min_size)
            result = 1;
        else if (sz > max_size)
            result = -1;
        return {result: result, extra: large_alloc_data};
    };
    
    for (let node_info of rbtree_iterator(root_node, node_evaluator)){
        let current_large_alloc_data = node_info.extra;
        let current_node_size = current_large_alloc_data.AllocatedPages * 0x1000;
        let current_node_address = current_large_alloc_data.VirtualAddress;
        let current_position = node_info.position;

        total_size += current_node_size;
        n++;
        logln(to_hex(current_node_address) + " sz: " + to_hex(current_node_size) + " d:" + current_position);
    }
    
    logln("\n++++++++++++++++++++++");
    logln("Total number of chunks: " + to_hex(n));
    logln("Total size: " + to_hex(total_size) + " (" + (total_size/(1024*1024)).toFixed(2) + " mb)");
    logln("------------------------\n");
}

function get_current_session_heap_state(){
    let current_session = host.currentProcess.KernelObject.Session;
    if ((current_session.address==zero) || (current_session.HeapState.address==zero))
        return 0;
    
    return host.createPointerObject(current_session.HeapState.address, "nt", "_EX_HEAP_SESSION_STATE*");
}

function set_session_pool(){
    globals = session_pool_globals;

    let heap_state = get_current_session_heap_state();

    if (!heap_state) {
        logln("No session pool found for the current process [" + host.currentProcess + "]");
        return;
    }
    
    globals.heap_manager = heap_state.HeapManager;
    globals.default_heap = {
        name: "SessionPool [" + host.currentProcess + "]",
        segment_heap: heap_state.PagedHeap,
        dynamic_lookaside: host.createPointerObject(heap_state.PagedHeap.UserContext.address, "nt", "_RTL_DYNAMIC_LOOKASIDE*"),
    };
    
    logln("Switching default pool to: " + globals.default_heap.name);
}

function set_default_pool(pool_index){
    if (pool_index == 4) {
        set_session_pool();
        return;
    }
    
    globals = kernel_globals;
    let selected_pool = globals.pools.find(pool => pool.pool_index == pool_index);

    if (selected_pool === undefined) {
        logln("Error setting default pool, available pool indexes:\n0: NonPagedPool\n1: NonPagedPoolNx\n2: PagedPool\n3: PagedProto\n4: Current Session Pool");
        return;
    }
    
    globals.default_heap = selected_pool;
    if ('heap_manager' in selected_pool) { //for session pool
        globals.heap_manager = selected_pool.heap_manager;
        globals.heap_manager_state = 0;
    }
    
    logln("Switching default pool to: " + selected_pool.name);
}

function _get_heap(segment_heap_addr){
    if (typeof segment_heap_addr === 'string'){
        segment_heap_addr = host.parseInt64(segment_heap_addr);
    }
    
    let format_name, segment_heap, dynamic_lookaside;

    if (is_userspace_address(segment_heap_addr)){
        format_name = (addr) => `usermode heap [${host.currentProcess} ${addr}]`;
        segment_heap = host.createPointerObject(segment_heap_addr, "ntdll", "_SEGMENT_HEAP*");
        
        //userspace segment heap currently has no dynamic lookaside, doesnt hurt to include it anyway
        dynamic_lookaside = host.createPointerObject(zero, "nt", "_RTL_DYNAMIC_LOOKASIDE*")
    } else {
        format_name = (addr) => `kernelmode heap ${addr}`;
        segment_heap = host.createPointerObject(segment_heap_addr, "nt", "_SEGMENT_HEAP*");
        dynamic_lookaside = host.createPointerObject(segment_heap.UserContext.address, "nt", "_RTL_DYNAMIC_LOOKASIDE*")
    }
        
    let name = format_name(to_hex(segment_heap_addr));
    logln("Switching segment heap to: " + name);
    
    return {
        name: name,
        segment_heap: segment_heap,
        dynamic_lookaside: dynamic_lookaside,
    }
}

function _set_globals(usermode){
    globals = kernel_globals;
    if (usermode){
        let new_globals = {};
        
        if (x("ld ntdll").indexOf("No modules matched") != -1){
            logln("reloading ntdll symbols....");
            x(".reload /f ntdll.dll");
        }
        
        new_globals.heap_manager_state = 0;
        new_globals.heap_manager = host.createPointerObject(host.getModuleSymbolAddress("ntdll", "RtlpHpHeapManager"), "ntdll", "_RTLP_HP_HEAP_MANAGER*");
        new_globals.RtlpHpHeapGlobals = host.createTypedObject(host.getModuleSymbolAddress("ntdll", "RtlpHpHeapGlobals"), "ntdll", "_RTLP_HP_HEAP_GLOBALS");
        new_globals.RtlpLfhBucketIndexMap = host.createTypedObject(host.getModuleSymbolAddress("ntdll", "RtlpLfhBucketIndexMap"), "nt", "unsigned char[]");
        new_globals.RtlpBucketBlockSizes = host.createTypedObject(host.getModuleSymbolAddress("ntdll", "RtlpBucketBlockSizes"), "nt", "unsigned short[]");
        
        new_globals.kernelmode = false;
        new_globals.default_module = "ntdll";
        new_globals.is_valid_address = is_userspace_address;
        
        globals = new_globals;
    }
}

function set_default_heap_seg(addr){
    _set_globals(is_userspace_address(addr));
    
    let seg_info = _seg_segment_info_blind(addr);
    if (seg_info === undefined){
        logln(`The ${to_hex(addr)} doesnt appear to be within a segment block`);
        return;
    }
    
    set_default_heap(seg_info.seg_context.Heap.address);
}

function set_default_heap(segment_heap_addr){
    _set_globals(is_userspace_address(segment_heap_addr));
    globals.default_heap = _get_heap(segment_heap_addr);
}

function set_kernel_pool_globals(){
    let heap_manager_state = host.createPointerObject(host.getModuleSymbolAddress("nt", "ExPoolState"), "nt", "_EX_POOL_HEAP_MANAGER_STATE*");
    let pool_node_heaps = heap_manager_state.PoolNode[0].Heaps;
    let pools = [];

    pools[0] = {
        pool_index: 0,
        name:"NonPagedPool Executable", 
    };
    
    pools[1] = {
        pool_index: 1,
        name:"NonPagedPoolNx", 
    };
    
    pools[2] = {
        pool_index: 2,
        name:"PagedPool",
    };
    
    pools[3] = {
        pool_index: 3,
        name:"PagedPoolProto", //seems to be used by nt!*Prototype* functions
    };
    
    for (let pool of pools) {
        pool.segment_heap = pool_node_heaps[pool.pool_index];
        pool.dynamic_lookaside = host.createPointerObject(pool.segment_heap.UserContext.address, "nt", "_RTL_DYNAMIC_LOOKASIDE*")
    }

    kernel_globals.pools = pools;
    kernel_globals.heap_manager = heap_manager_state.HeapManager;

    kernel_globals.RtlpHpHeapGlobals = host.createTypedObject(host.getModuleSymbolAddress("nt", "RtlpHpHeapGlobals"), "nt", "_RTLP_HP_HEAP_GLOBALS");
    kernel_globals.RtlpLfhBucketIndexMap = host.createTypedObject(host.getModuleSymbolAddress("nt", "RtlpLfhBucketIndexMap"), "nt", "unsigned char[]");
    kernel_globals.RtlpBucketBlockSizes = host.createTypedObject(host.getModuleSymbolAddress("nt", "RtlpBucketBlockSizes"), "nt", "unsigned short[]");
    kernel_globals.dynamic_lookaside_max_size = kernel_globals.RtlpBucketBlockSizes[0x21 + pools[1].dynamic_lookaside.BucketCount - 1]; //assuming all kernel heaps have the same max size
    kernel_globals.dynamic_lookaside_min_size = pools[1].segment_heap.LfhContext.Config.MaxBlockSize+type_size("nt!_POOL_HEADER"); //assuming the dynamic lookaside begins where the LFH ends
    
    kernel_globals.default_module = "nt";
    kernel_globals.is_valid_address = is_kernelspace_address;
    kernel_globals.kernelmode = true;
}

function invokeScript(){
    check_machine_configuration();
    set_kernel_pool_globals();
    
    //ideally we want deep copy, but this should be good enough
    session_pool_globals = Object.assign({}, kernel_globals);
    
    set_default_pool(1);
}